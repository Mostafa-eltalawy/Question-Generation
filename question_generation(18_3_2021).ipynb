{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "question_generation(18-3-2021).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mostafa-eltalawy/Question-Generation/blob/master/question_generation(18_3_2021).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aweZgxXBDsOQ"
      },
      "source": [
        "#!pip install -U transformers==3.0.0\n",
        "#!pip install PIL\n",
        "#!pip install pytesseract\n",
        "#!pip install pdf2image\n",
        "#!sudo apt-get install tesseract-ocr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FARi6xuQ4IZ"
      },
      "source": [
        "#!python -m nltk.downloader punkt\n",
        "#!pip install allennlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-TLSOF7Hsbo"
      },
      "source": [
        "import itertools\n",
        "import logging\n",
        "from typing import Optional, Dict, Union\n",
        "\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "import torch\n",
        "from transformers import(\n",
        "    AutoModelForSeq2SeqLM, \n",
        "    AutoTokenizer,\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer,)\n",
        "\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "logger = logging.getLogger(__name__)\n",
        "# Importing package and summarizer\n",
        "from gensim.summarization.summarizer import summarize\n",
        "from gensim.summarization import keywords\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "import spacy\n",
        "from spacy.matcher import Matcher \n",
        "\n",
        "from spacy import displacy \n",
        "#import visualise_spacy_tree\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# load english language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# Import libraries for pdf reading\n",
        "from PIL import Image \n",
        "import pytesseract \n",
        "import sys \n",
        "from pdf2image import convert_from_path \n",
        "import os \n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "from collections import Counter\n",
        "import en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSBLRJ43OB3T"
      },
      "source": [
        "#reading pdf file and converting it to text file by OCR\n",
        "# Path of the pdf \n",
        "PDF_file = \"Invest-Law.pdf\"\n",
        "\n",
        "''' \n",
        "Part #1 : Converting PDF to images \n",
        "'''\n",
        "# Store all the pages of the PDF in a variable \n",
        "pages = convert_from_path(PDF_file, 500) \n",
        "# Counter to store images of each page of PDF to image \n",
        "image_counter = 1\n",
        "# Iterate through all the pages stored above \n",
        "for page in pages: \n",
        "\tfilename = \"page_\"+str(image_counter)+\".jpg\"\n",
        "\t# Save the image of the page in system \n",
        "\tpage.save(filename, 'JPEG') \n",
        "\t# Increment the counter to update filename \n",
        "\timage_counter = image_counter + 1\n",
        "\n",
        "''' \n",
        "Part #2 - Recognizing text from the images using OCR \n",
        "'''\n",
        "# Variable to get count of total number of pages \n",
        "filelimit = image_counter-1\n",
        "# Creating a text file to write the output \n",
        "outfile = \"out_text.txt\"\n",
        "# Open the file in append mode so that All contents of all images are added to the same file \n",
        "textfile = open(outfile, \"a\") \n",
        "# Iterate from 1 to total number of pages \n",
        "for i in range(1, filelimit + 1): \n",
        "\tfilename = \"page_\"+str(i)+\".jpg\"\t\n",
        "\t# Recognize the text as string in image using pytesserct \n",
        "\ttext = str(((pytesseract.image_to_string(Image.open(filename))))) \n",
        "\ttext = text.replace('-\\n', '')\t \n",
        "\t# Finally, write the processed text to the file. \n",
        "\ttextfile.write(text) \n",
        "textfile.close() \n",
        "\n",
        "f = open(\"out_text\", \"r\")\n",
        "inputtext = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIKYlhRFIfIh"
      },
      "source": [
        "class Question_Generation_Pipeline:\n",
        "    \"\"\"Poor man's QG pipeline\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: PreTrainedModel,\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        ans_model: PreTrainedModel,\n",
        "        ans_tokenizer: PreTrainedTokenizer,\n",
        "        qg_format: str,\n",
        "        use_cuda: bool\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.ans_model = ans_model\n",
        "        self.ans_tokenizer = ans_tokenizer\n",
        "\n",
        "        self.qg_format = qg_format\n",
        "\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        if self.ans_model is not self.model:\n",
        "            self.ans_model.to(self.device)\n",
        "\n",
        "        assert self.model.__class__.__name__ in [\"T5ForConditionalGeneration\", \"BartForConditionalGeneration\"]\n",
        "        \n",
        "        if \"T5ForConditionalGeneration\" in self.model.__class__.__name__:\n",
        "            self.model_type = \"t5\"\n",
        "        else:\n",
        "            self.model_type = \"bart\"\n",
        "\n",
        "    def __call__(self, inputs: str):\n",
        "        inputs = \" \".join(inputs.split())\n",
        "        sents, answers = self._extract_answers(inputs)\n",
        "        flat_answers = list(itertools.chain(*answers))\n",
        "        \n",
        "        if len(flat_answers) == 0:\n",
        "          return []\n",
        "\n",
        "        if self.qg_format == \"prepend\":\n",
        "            qg_examples = self._prepare_inputs_for_qg_from_answers_prepend(inputs, answers)\n",
        "        else:\n",
        "            qg_examples = self._prepare_inputs_for_qg_from_answers_hl(sents, answers)\n",
        "        \n",
        "        qg_inputs = [example['source_text'] for example in qg_examples]\n",
        "        questions = self._generate_questions(qg_inputs)\n",
        "        output = [{'answer': example['answer'], 'question': que} for example, que in zip(qg_examples, questions)]\n",
        "        return output\n",
        "    \n",
        "    def _generate_questions(self, inputs):\n",
        "        inputs = self._tokenize(inputs, padding=True, truncation=True)\n",
        "        \n",
        "        outs = self.model.generate(\n",
        "            input_ids=inputs['input_ids'].to(self.device), \n",
        "            attention_mask=inputs['attention_mask'].to(self.device), \n",
        "            max_length=32,\n",
        "            num_beams=4,\n",
        "        )\n",
        "        \n",
        "        questions = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
        "        return questions\n",
        "    \n",
        "    def _extract_answers(self, context):\n",
        "        sents, inputs = self._prepare_inputs_for_ans_extraction(context)\n",
        "        inputs = self._tokenize(inputs, padding=True, truncation=True)\n",
        "\n",
        "        outs = self.ans_model.generate(\n",
        "            input_ids=inputs['input_ids'].to(self.device), \n",
        "            attention_mask=inputs['attention_mask'].to(self.device), \n",
        "            max_length=32,\n",
        "        )\n",
        "        \n",
        "        dec = [self.ans_tokenizer.decode(ids, skip_special_tokens=False) for ids in outs]\n",
        "        answers = [item.split('<sep>') for item in dec]\n",
        "        answers = [i[:-1] for i in answers]\n",
        "        \n",
        "        return sents, answers\n",
        "    \n",
        "    def _tokenize(self,\n",
        "        inputs,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        add_special_tokens=True,\n",
        "        max_length=512 \n",
        "    ):\n",
        "        inputs = self.tokenizer.batch_encode_plus(\n",
        "            inputs, \n",
        "            max_length=max_length,\n",
        "            add_special_tokens=add_special_tokens,\n",
        "            truncation=truncation,\n",
        "            padding=\"max_length\" if padding else False,\n",
        "            pad_to_max_length=padding,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return inputs\n",
        "    \n",
        "    def _prepare_inputs_for_ans_extraction(self, text):\n",
        "        sents = sent_tokenize(text)\n",
        "\n",
        "        inputs = []\n",
        "        for i in range(len(sents)):\n",
        "            source_text = \"extract answers:\"\n",
        "            for j, sent in enumerate(sents):\n",
        "                if i == j:\n",
        "                    sent = \"<hl> %s <hl>\" % sent\n",
        "                source_text = \"%s %s\" % (source_text, sent)\n",
        "                source_text = source_text.strip()\n",
        "            \n",
        "            if self.model_type == \"t5\":\n",
        "                source_text = source_text + \" </s>\"\n",
        "            inputs.append(source_text)\n",
        "\n",
        "        return sents, inputs\n",
        "    \n",
        "    def _prepare_inputs_for_qg_from_answers_hl(self, sents, answers):\n",
        "        inputs = []\n",
        "        for i, answer in enumerate(answers):\n",
        "            if len(answer) == 0: continue\n",
        "            for answer_text in answer:\n",
        "                sent = sents[i]\n",
        "                sents_copy = sents[:]\n",
        "                \n",
        "                answer_text = answer_text.strip()\n",
        "                \n",
        "                ans_start_idx = sent.index(answer_text)\n",
        "                \n",
        "                sent = f\"{sent[:ans_start_idx]} <hl> {answer_text} <hl> {sent[ans_start_idx + len(answer_text): ]}\"\n",
        "                sents_copy[i] = sent\n",
        "                \n",
        "                source_text = \" \".join(sents_copy)\n",
        "                source_text = f\"generate question: {source_text}\" \n",
        "                if self.model_type == \"t5\":\n",
        "                    source_text = source_text + \" </s>\"\n",
        "                \n",
        "                inputs.append({\"answer\": answer_text, \"source_text\": source_text})\n",
        "        \n",
        "        return inputs\n",
        "    \n",
        "    def _prepare_inputs_for_qg_from_answers_prepend(self, context, answers):\n",
        "        flat_answers = list(itertools.chain(*answers))\n",
        "        examples = []\n",
        "        for answer in flat_answers:\n",
        "            source_text = f\"answer: {answer} context: {context}\"\n",
        "            if self.model_type == \"t5\":\n",
        "                source_text = source_text + \" </s>\"\n",
        "            \n",
        "            examples.append({\"answer\": answer, \"source_text\": source_text})\n",
        "        return examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai4iXkG1IevW"
      },
      "source": [
        "class MultiTask_QAQG_Pipeline(Question_Generation_Pipeline):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "    \n",
        "    def __call__(self, inputs: Union[Dict, str]):\n",
        "        if type(inputs) is str:\n",
        "            # do qg\n",
        "            return super().__call__(inputs)\n",
        "        else:\n",
        "            # do qa\n",
        "            return self._extract_answer(inputs[\"question\"], inputs[\"context\"])\n",
        "    \n",
        "    def _prepare_inputs_for_qa(self, question, context):\n",
        "        source_text = f\"question: {question}  context: {context}\"\n",
        "        if self.model_type == \"t5\":\n",
        "            source_text = source_text + \" </s>\"\n",
        "        return  source_text\n",
        "    \n",
        "    def _extract_answer(self, question, context):\n",
        "        source_text = self._prepare_inputs_for_qa(question, context)\n",
        "        inputs = self._tokenize([source_text], padding=False)\n",
        "    \n",
        "        outs = self.model.generate(\n",
        "            input_ids=inputs['input_ids'].to(self.device), \n",
        "            attention_mask=inputs['attention_mask'].to(self.device), \n",
        "            max_length=16, #changed from 16 to 10 \n",
        "        )\n",
        "\n",
        "        answer = self.tokenizer.decode(outs[0], skip_special_tokens=True)\n",
        "        return answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soAeTGVYIhGb"
      },
      "source": [
        "class E2E_Question_Generation_Pipeline:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: PreTrainedModel,\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        use_cuda: bool\n",
        "    ) :\n",
        "\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        assert self.model.__class__.__name__ in [\"T5ForConditionalGeneration\", \"BartForConditionalGeneration\"]\n",
        "        \n",
        "        if \"T5ForConditionalGeneration\" in self.model.__class__.__name__:\n",
        "            self.model_type = \"t5\"\n",
        "        else:\n",
        "            self.model_type = \"bart\"\n",
        "        \n",
        "        self.default_generate_kwargs = {\n",
        "            \"max_length\": 256, #changed from 256 to 10\n",
        "            \"num_beams\": 4,\n",
        "            \"length_penalty\": 1.5,\n",
        "            \"no_repeat_ngram_size\": 3,\n",
        "            \"early_stopping\": True,\n",
        "        }\n",
        "    \n",
        "    def __call__(self, context: str, **generate_kwargs):\n",
        "        inputs = self._prepare_inputs_for_e2e_qg(context)\n",
        "\n",
        "        # TODO: when overrding default_generate_kwargs all other arguments need to be passsed\n",
        "        # find a better way to do this\n",
        "        if not generate_kwargs:\n",
        "            generate_kwargs = self.default_generate_kwargs\n",
        "        \n",
        "        input_length = inputs[\"input_ids\"].shape[-1]\n",
        "        \n",
        "        # max_length = generate_kwargs.get(\"max_length\", 256)\n",
        "        # if input_length < max_length:\n",
        "        #     logger.warning(\n",
        "        #         \"Your max_length is set to {}, but you input_length is only {}. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\".format(\n",
        "        #             max_length, input_length\n",
        "        #         )\n",
        "        #     )\n",
        "\n",
        "        outs = self.model.generate(\n",
        "            input_ids=inputs['input_ids'].to(self.device), \n",
        "            attention_mask=inputs['attention_mask'].to(self.device),\n",
        "            **generate_kwargs\n",
        "        )\n",
        "\n",
        "        prediction = self.tokenizer.decode(outs[0], skip_special_tokens=True)\n",
        "        questions = prediction.split(\"<sep>\")\n",
        "        questions = [question.strip() for question in questions[:-1]]\n",
        "        return questions\n",
        "    \n",
        "    def _prepare_inputs_for_e2e_qg(self, context):\n",
        "        source_text = f\"generate questions: {context}\"\n",
        "        if self.model_type == \"t5\":\n",
        "            source_text = source_text + \" </s>\"\n",
        "        \n",
        "        inputs = self._tokenize([source_text], padding=False)\n",
        "        return inputs\n",
        "    \n",
        "    def _tokenize(\n",
        "        self,\n",
        "        inputs,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        add_special_tokens=True,\n",
        "        max_length=512\n",
        "    ):\n",
        "        inputs = self.tokenizer.batch_encode_plus(\n",
        "            inputs, \n",
        "            max_length=max_length,\n",
        "            add_special_tokens=add_special_tokens,\n",
        "            truncation=truncation,\n",
        "            padding=\"max_length\" if padding else False,\n",
        "            pad_to_max_length=padding,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxek15Y0RzSk"
      },
      "source": [
        "modelPath_e2e_qg_small='valhalla/t5-small-e2e-qg/'  # valhalla folder must be at the same directory with the code\n",
        "modelPath_qg_small='valhalla/t5-small-qg-hl/'\n",
        "modelPath_qa_qg_small='valhalla/t5-small-qa-qg-hl/'\n",
        "\n",
        "modelPath_e2e_qg_base='valhalla/t5-base-e2e-qg/'\n",
        "modelPath_qg_base='valhalla/t5-base-qg-hl/'\n",
        "modelPath_qa_qg_base='valhalla/t5-base-qa-qg-hl/'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OG3BnUgR7bb"
      },
      "source": [
        "#downloading and saving model files one time only\n",
        "model.save_pretrained(modelPath_e2e_qg_small)  #downloading model and save it to folder valhalla then folder t5-small-e2e-qg\n",
        "model.save_pretrained(modelPath_qg_small)  #downloading model and save it to folder valhalla then folder t5-small-qg-hl\n",
        "model.save_pretrained(modelPath_qa_qg_small)  #downloading model and save it to folder valhalla then folder t5-small-qa-qg-hl\n",
        "model.save_pretrained(modelPath_e2e_qg_base)  #downloading model and save it to folder valhalla then folder t5-base-e2e-qg\n",
        "model.save_pretrained(modelPath_qg_base)  #downloading model and save it to folder valhalla then folder t5-base-qg-hl\n",
        "model.save_pretrained(modelPath_qa_qg_base)  #downloading model and save it to folder valhalla then folder t5-base-qa-qg-hl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0UoeoqU4XoL"
      },
      "source": [
        "#tokenizer.save_pretrained(modelPath_e2e_qg)   #downloading model and save it to folder valhalla then folder t5-small-e2e-qg\n",
        "#tokenizer = AutoTokenizer.from_pretrained(modelPath_qa_qg_small)\n",
        "#model = AutoModelForSeq2SeqLM.from_pretrained(modelPath_qa_qg_small)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMTGTRKsHs49"
      },
      "source": [
        "SUPPORTED_TASKS = {\n",
        "    \"question-generation\": {\n",
        "        \"impl\": Question_Generation_Pipeline,\n",
        "        \"default\": {\n",
        "            \"model\": modelPath_qg_small,\n",
        "            \"ans_model\": modelPath_qa_qg_small,\n",
        "        }\n",
        "    },\n",
        "    \"multitask-qa-qg\": {\n",
        "        \"impl\": MultiTask_QAQG_Pipeline,\n",
        "        \"default\": {\n",
        "            \"model\": modelPath_qa_qg_small,\n",
        "        }\n",
        "    },\n",
        "    \"e2e-qg\": {\n",
        "        \"impl\": E2E_Question_Generation_Pipeline,\n",
        "        \"default\": {\n",
        "            \"model\": modelPath_e2e_qg_small,\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKZfGCfDHvnq"
      },
      "source": [
        "def GenAnsPipline(\n",
        "    task: str,\n",
        "    model: Optional = None,\n",
        "    tokenizer: Optional[Union[str, PreTrainedTokenizer]] = None,\n",
        "    qg_format: Optional[str] = \"highlight\",\n",
        "    ans_model: Optional = None,\n",
        "    ans_tokenizer: Optional[Union[str, PreTrainedTokenizer]] = None,\n",
        "    use_cuda: Optional[bool] = True,\n",
        "    **kwargs,\n",
        "):\n",
        "    # Retrieve the task\n",
        "    if task not in SUPPORTED_TASKS:\n",
        "        raise KeyError(\"Unknown task {}, available tasks are {}\".format(task, list(SUPPORTED_TASKS.keys())))\n",
        "\n",
        "    targeted_task = SUPPORTED_TASKS[task]\n",
        "    task_class = targeted_task[\"impl\"]\n",
        "\n",
        "    # Use default model/config/tokenizer for the task if no model is provided\n",
        "    if model is None:\n",
        "        model = targeted_task[\"default\"][\"model\"]\n",
        "    \n",
        "    # Try to infer tokenizer from model or config name (if provided as str)\n",
        "    if tokenizer is None:\n",
        "        if isinstance(model, str):\n",
        "            tokenizer = model\n",
        "        else:\n",
        "            # Impossible to guest what is the right tokenizer here\n",
        "            raise Exception(\n",
        "                \"Impossible to guess which tokenizer to use. \"\n",
        "                \"Please provided a PretrainedTokenizer class or a path/identifier to a pretrained tokenizer.\"\n",
        "            )\n",
        "    \n",
        "    # Instantiate tokenizer if needed\n",
        "    if isinstance(tokenizer, (str, tuple)):\n",
        "        if isinstance(tokenizer, tuple):\n",
        "            # For tuple we have (tokenizer name, {kwargs})\n",
        "            tokenizer = AutoTokenizer.from_pretrained(tokenizer[0], **tokenizer[1])\n",
        "        else:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
        "    \n",
        "    # Instantiate model if needed\n",
        "    if isinstance(model, str):\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model)\n",
        "    \n",
        "    if task == \"question-generation\":\n",
        "        if ans_model is None:\n",
        "            # load default ans model\n",
        "            ans_model = targeted_task[\"default\"][\"ans_model\"]\n",
        "            ans_tokenizer = AutoTokenizer.from_pretrained(ans_model)\n",
        "            ans_model = AutoModelForSeq2SeqLM.from_pretrained(ans_model)\n",
        "        else:\n",
        "            # Try to infer tokenizer from model or config name (if provided as str)\n",
        "            if ans_tokenizer is None:\n",
        "                if isinstance(ans_model, str):\n",
        "                    ans_tokenizer = ans_model\n",
        "                else:\n",
        "                    # Impossible to guest what is the right tokenizer here\n",
        "                    raise Exception(\n",
        "                        \"Impossible to guess which tokenizer to use. \"\n",
        "                        \"Please provided a PretrainedTokenizer class or a path/identifier to a pretrained tokenizer.\"\n",
        "                    )\n",
        "            \n",
        "            # Instantiate tokenizer if needed\n",
        "            if isinstance(ans_tokenizer, (str, tuple)):\n",
        "                if isinstance(ans_tokenizer, tuple):\n",
        "                    # For tuple we have (tokenizer name, {kwargs})\n",
        "                    ans_tokenizer = AutoTokenizer.from_pretrained(ans_tokenizer[0], **ans_tokenizer[1])\n",
        "                else:\n",
        "                    ans_tokenizer = AutoTokenizer.from_pretrained(ans_tokenizer)\n",
        "\n",
        "            if isinstance(ans_model, str):\n",
        "                ans_model = AutoModelForSeq2SeqLM.from_pretrained(ans_model)\n",
        "    \n",
        "    if task == \"e2e-qg\":\n",
        "        return task_class(model=model, tokenizer=tokenizer, use_cuda=use_cuda)\n",
        "    elif task == \"question-generation\":\n",
        "        return task_class(model=model, tokenizer=tokenizer, ans_model=ans_model, ans_tokenizer=ans_tokenizer, qg_format=qg_format, use_cuda=use_cuda)\n",
        "    else:\n",
        "        return task_class(model=model, tokenizer=tokenizer, ans_model=model, ans_tokenizer=tokenizer, qg_format=qg_format, use_cuda=use_cuda)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvBFbCVGELuW"
      },
      "source": [
        "# qenerating and answering task loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFSZiIc0StHY"
      },
      "source": [
        "#generating Questions using model=\"valhalla/t5-small-qg-hl\"\n",
        "QGSmall = GenAnsPipline(\"question-generation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEZ5CHOKlyHF"
      },
      "source": [
        "#generating Questions using model=\"valhalla/t5-base-qg-hl\"\n",
        "QGBase = GenAnsPipline(\"question-generation\", model=modelPath_qg_base)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAkVmsH9VEIu"
      },
      "source": [
        "#generating Questions and answering  using model=\"valhalla/t5-small-qa-qg-hl\"\n",
        "MultiTaskQAQGsmall = GenAnsPipline(\"multitask-qa-qg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx1KjJzaWa-a"
      },
      "source": [
        "#generating Questions and answering  using model=\"valhalla/t5-base-qa-qg-hl\"\n",
        "MultiTaskQAQGbase = GenAnsPipline(\"multitask-qa-qg\", model=modelPath_qa_qg_base)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzJjXzpmXG7t"
      },
      "source": [
        "#generating Questions using model=\"valhalla/t5-small-e2e-qg\"\n",
        "E2EQGsmall = GenAnsPipline(\"e2e-qg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5oUJ5_0X_Je"
      },
      "source": [
        "#generating Questions using model=\"valhalla/t5-base-e2e-qg\"\n",
        "E2EQGbase = GenAnsPipline(\"e2e-qg\", model=modelPath_e2e_qg_base)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dtHH-_tG2H9"
      },
      "source": [
        "# function to clean text\n",
        "def clean(text):\n",
        "    # removing paragraph numbers\n",
        "    #text = re.sub('[0-9]+.\\t','',str(text))\n",
        "    # removing new line characters\n",
        "    text = re.sub('\\n ','',str(text))\n",
        "    text = re.sub('\\n',' ',str(text))\n",
        "    # removing apostrophes\n",
        "    text = re.sub(\"'s\",'',str(text))\n",
        "    # removing hyphens\n",
        "    text = re.sub(\"-\",' ',str(text))\n",
        "    text = re.sub(\"— \",'',str(text))\n",
        "    # removing quotation marks\n",
        "    text = re.sub('\\\"','',str(text))\n",
        "    # removing salutations\n",
        "    text = re.sub(\"Mr\\.\",'Mr',str(text))\n",
        "    text = re.sub(\"Mrs\\.\",'Mrs',str(text))\n",
        "    # removing any reference to outside text\n",
        "    text = re.sub(\"[\\\\[]*[\\\\]]\", \"\", str(text))\n",
        "    \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc27i7y3ogF3"
      },
      "source": [
        "# split inputtext to several sentences \n",
        "def sentences(text):\n",
        "  text = re.split('[.?]', text)\n",
        "  clean_sent = []\n",
        "  for sent in text:\n",
        "      clean_sent.append(sent)\n",
        "  return clean_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1n2gsR8EH5B"
      },
      "source": [
        "def GetQuestionsAndAnswers(inputtext):\n",
        "  cleanedtext=clean(inputtext)\n",
        "  listedtext=sentences(inputtext)\n",
        "  questions1=E2EQGsmall(listedtext)\n",
        "  questions2=E2EQGbase(listedtext)\n",
        "  #print('questions list is : \\n' , questions)\n",
        "  #print('====================================================================')\n",
        "  for question in questions1:\n",
        "    answer1 = MultiTaskQAQGsmall({\"question\": question, \"context\": cleanedtext})\n",
        "    answer2 = MultiTaskQAQGbase({\"question\": question, \"context\": cleanedtext})\n",
        "    print( ' question (small model) is : ' ,question  )\n",
        "    print( ' answer1 (small model) is : ' ,answer1  )\n",
        "    print( ' answer2 (base model) is : ' ,answer2  )\n",
        "    print('------------------------------------------------------------------')\n",
        "  print('=====================================================================================')\n",
        "  for question in questions2:\n",
        "    answer1 = MultiTaskQAQGsmall({\"question\": question, \"context\": cleanedtext})\n",
        "    answer2 = MultiTaskQAQGbase({\"question\": question, \"context\": cleanedtext})\n",
        "    print( ' question (base model) is : ' ,question  )\n",
        "    print( ' answer1  (small model) is : ' ,answer1  )\n",
        "    print( ' answer2 (base model) is : ' ,answer2  )\n",
        "    print('------------------------------------------------------------------')\n",
        "  #return questions , answer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAOQH6f69RBp"
      },
      "source": [
        "def getkeywordsEntSum(inputtext):\n",
        "  textKeywords=keywords(inputtext, words=20, lemmatize=True)\n",
        "  print(textKeywords)\n",
        "  summerizedtext=summarize(inputtext, ratio=0.6)\n",
        "  print(summerizedtext)\n",
        "  doc=nlp(inputtext)\n",
        "  for ent in doc.ents: \n",
        "    print(ent.text, ' .. and word type is ..',  ent.label_)\n",
        "  enydisplay = displacy.render(nlp(str(inputtext)), jupyter=True, style='ent') \n",
        "  print(enydisplay) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmDW4comB74v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}